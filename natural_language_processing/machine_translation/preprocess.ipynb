{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Motivated by [fast.ai](fast.ai)\n",
    "\n",
    "This notebook takes the matched sentences from the [2015 EMNLP shared task](http://www.statmt.org/wmt15/translation-task.html) (specifically the [Giga French English corpus](http://www.statmt.org/wmt10/training-giga-fren.tar)), and tokenizes them.\n",
    "\n",
    "Only who, what, why, when, where questions are tokenized (i.e. if the English sentence starts with `Wh` and ends with `?`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate.data.process import QuestionTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "french, english = Path('data/giga-fren.release2.fixed.fr'), Path('data/giga-fren.release2.fixed.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer's constructor takes as input a tuple `(path_to_english_sentences, path_to_french_sentences)`. In addition, the number of processes, parallelism and number of chunks the data is split into can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49881 questions\n"
     ]
    }
   ],
   "source": [
    "tokenizer = QuestionTokenizer((english, french))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer's `preprocess` method by default assumes a maximum vocabulary of 40,000. This can be modified with the `vocab_size` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing english questions\n",
      "Processed 10000 articles\n",
      "Processed 20000 articles\n",
      "Processed 30000 articles\n",
      "Processed 40000 articles\n",
      "Tokenizing french questions\n",
      "Processed 10000 articles\n",
      "Processed 20000 articles\n",
      "Processed 30000 articles\n",
      "Processed 40000 articles\n",
      "Tokenized questions!\n"
     ]
    }
   ],
   "source": [
    "english, french = tokenizer.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ints, en_dict = english\n",
    "fr_ints, fr_dict = french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('en_ints.npy', np.array(en_ints))\n",
    "np.save('fr_ints.npy', np.array(fr_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = Path('en_word2int.pickle')\n",
    "fr_path = Path('fr_word2int.pickle')\n",
    "\n",
    "with en_path.open(mode='wb') as en_file:\n",
    "    pickle.dump(en_dict, en_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with fr_path.open(mode='wb') as fr_file:\n",
    "    pickle.dump(fr_dict, fr_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check, to make sure the order is okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentence(word2int, ints):\n",
    "    int2word = {int(idx): word for word, idx in word2int.items()}\n",
    "    \n",
    "    return \" \".join([int2word[i] for i in ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qu’ est -ce que la lumière ? xeos\n",
      "what is light ? xeos\n",
      "-------\n",
      "où sommes - nous ? xeos\n",
      "who are we ? xeos\n",
      "-------\n",
      "t_up d' où venons - nous ? xeos\n",
      "where did we come from ? xeos\n",
      "-------\n",
      "que ferions - nous sans elle ? xeos\n",
      "what would we do without it ? xeos\n",
      "-------\n",
      "quelle sont les coordonnées ( latitude et longitude ) de badger , à terre - neuve - etlabrador ? xeos\n",
      "what is the absolute location ( latitude and longitude ) of badger , newfoundland and labrador ? xeos\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(read_sentence(fr_dict, fr_ints[i]))\n",
    "    print(read_sentence(en_dict, en_ints[i]))\n",
    "    print('-------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
