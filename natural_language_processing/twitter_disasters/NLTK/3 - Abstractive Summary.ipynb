{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "# warning: the below makes unicode the default\n",
    "from __future__ import unicode_literals \n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import nltk\n",
    "import inspect\n",
    "\n",
    "from textacy.vsm import Vectorizer\n",
    "import textacy.vsm\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "import kenlm\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cowts_tweets = pd.read_pickle('cowts_nltk_tweets.pkl')\n",
    "term_matrix = np.load('term_matrix_nltk.npy')\n",
    "vocab_to_idx = np.load('vocab_to_idx_nltk.npy').item()\n",
    "\n",
    "content_vocab = list(np.load('content_vocab_nltk.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing it using NLTK's twitter tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\": LATEST Nepal's Kantipur TV shows at least 21 bodies lined up on ground after 7.9 earthquake\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cowts_tweets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u':', u'LATEST', u\"Nepal's\", u'Kantipur', u'TV', u'shows', u'at', u'least', u'21', u'bodies', u'lined', u'up', u'on', u'ground', u'after', u'7.9', u'earthquake']\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.tokenize(cowts_tweets[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_tweets = []\n",
    "\n",
    "for tweet in cowts_tweets[0]:\n",
    "    nltk_tweets.append(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these 59 tweets, which consist of a 1000 word summary of the dataset, I want to generate a paragraph summary from the words. I am going to do this in two steps: \n",
    "\n",
    "1. First, I am going to generate a bigram of the words. This will map all of the words in all of the tweets to all the other words they are connected to. \n",
    "\n",
    "2. Then, I am going to use the bigram to find an optimal 'text path', which I will solve using Integer Linear Programming. \n",
    "\n",
    "Some of the tweets are single word tweets (eg. '`Awful`'). These will not contribute anything to the word graph, so I remove them from this list of chosen tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_tweets = [tweet for tweet in nltk_tweets if len(tweet) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a word graph\n",
    "\n",
    "I want to make a bigram; each node of a bigram is two adjacent words in a tweet. I can then generate sentences by traversing paths (which lead to the same word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for a single tweet, a bigram would look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u':', u'LATEST'),\n",
       " (u'LATEST', u\"Nepal's\"),\n",
       " (u\"Nepal's\", u'Kantipur'),\n",
       " (u'Kantipur', u'TV'),\n",
       " (u'TV', u'shows'),\n",
       " (u'shows', u'at'),\n",
       " (u'at', u'least'),\n",
       " (u'least', u'21'),\n",
       " (u'21', u'bodies'),\n",
       " (u'bodies', u'lined'),\n",
       " (u'lined', u'up'),\n",
       " (u'up', u'on'),\n",
       " (u'on', u'ground'),\n",
       " (u'ground', u'after'),\n",
       " (u'after', u'7.9'),\n",
       " (u'7.9', u'earthquake')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(bigrams(nltk_tweets[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now construct this for all the tweets, simply by adding their respective bigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_bigrams = [list(bigrams([token for token in tweets])) for tweets in nltk_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I take the the starting and end nodes of the bigrams, so that I can generate word paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "starting_nodes = [single_bigram[0] for single_bigram in all_bigrams]\n",
    "end_nodes = [single_bigram[-1] for single_bigram in all_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_bigrams = [node for single_bigram in all_bigrams for node in single_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_bigrams = list(set(all_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bigrams themselves are not super useful; what I want to do is compile a list of all the word paths through the bigram. I can do this by using the starts of the tweets as the 'beginnings', and the ends as 'ends'. \n",
    "\n",
    "In order to limit the number of word paths, I will limit the path length to between 10 and 15 paths. I'm going to implement a breadth first search to find these word paths.\n",
    "\n",
    "The first step is to take my bigram list and turn it into a dictionary; this will make it easier to find the paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_bigram_graph(all_bigrams, start_node):\n",
    "    bigram = all_bigrams[:]\n",
    "    \n",
    "    '''\n",
    "    Given a bigram, with a defined start node and defined end nodes, this method\n",
    "    returns a dictionary which serves as a graph for that bigram \n",
    "    '''\n",
    "    def find_children(bigram, node):\n",
    "        '''\n",
    "        Given a node, this method finds all its children \n",
    "        '''\n",
    "        second_word = node[1]\n",
    "        \n",
    "        children = [node for node in bigram if node[0] == second_word]\n",
    "        \n",
    "        return children\n",
    "   \n",
    "    bigram_graph = {}\n",
    "    # start by adding the start node\n",
    "    bigram_graph[start_node] = find_children(all_bigrams, start_node)\n",
    "    bigram.remove(start_node)\n",
    "    \n",
    "    nodes_to_check = []\n",
    "    for i in find_children(bigram, start_node):\n",
    "        nodes_to_check.append(i)\n",
    "        \n",
    "    while nodes_to_check: \n",
    "        node = nodes_to_check.pop()\n",
    "        if node in bigram: \n",
    "            bigram_graph[node] = find_children(bigram, node)\n",
    "            bigram.remove(node)\n",
    "            for i in find_children(bigram, node):\n",
    "                nodes_to_check.append(i)\n",
    "    return bigram_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_graph = make_bigram_graph(all_bigrams, starting_nodes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have this dictionary, I am going to implement a breadth first search to find all possible paths between a start node and an end node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def breadth_first_search(bigram_graph, start_node, end_node):\n",
    "    '''\n",
    "    This method takes as input a graph, a start node and an end node\n",
    "    and returns all paths which have a length between 10 and 16\n",
    "    between the two nodes.\n",
    "    '''\n",
    "    graph_to_manipulate = dict(bigram_graph)\n",
    "    \n",
    "    queue = []\n",
    "    paths_to_return = []\n",
    "    queue.append([start_node])\n",
    "    \n",
    "    while queue:\n",
    "        # get the first path from the queue\n",
    "        path = queue.pop(0)\n",
    "        # get the last node from the path\n",
    "        node = path[-1]\n",
    "        # path found\n",
    "        if node == end_node:\n",
    "            if (len(path) < 16) and (len(path) > 10): #limit path length \n",
    "                paths_to_return.append(path)\n",
    "        # enumerate all adjacent nodes, construct a new path and push it into the queue\n",
    "        for adjacent in graph_to_manipulate.get(node, []):\n",
    "            new_path = list(path)\n",
    "            new_path.append(adjacent)\n",
    "            queue.append(new_path)\n",
    "        if node in graph_to_manipulate: \n",
    "            del graph_to_manipulate[node] # prevents circular references\n",
    "\n",
    "    return paths_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = breadth_first_search(bigram_graph, starting_nodes[1], end_nodes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the first path this produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MEA', u'opens'),\n",
       " (u'opens', u'24hr'),\n",
       " (u'24hr', u'Control'),\n",
       " (u'Control', u'Room'),\n",
       " (u'Room', u'in'),\n",
       " (u'in', u'Kathmandu'),\n",
       " (u'Kathmandu', u':'),\n",
       " (u':', u'Please'),\n",
       " (u'Please', u'Retweet'),\n",
       " (u'Retweet', u'.'),\n",
       " (u'.', u'\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a word path. Success! \n",
    "\n",
    "Now, I just need to repeat this exercise for every starting node, mapping to every end node, to collect a 'total word path' document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:24<00:00,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "bigram_paths = []\n",
    "\n",
    "for single_start_node in tqdm(starting_nodes): \n",
    "    bigram_graph = make_bigram_graph(all_bigrams, single_start_node)\n",
    "    for single_end_node in end_nodes:\n",
    "        possible_paths = breadth_first_search(bigram_graph, single_start_node, single_end_node)\n",
    "        for path in possible_paths: \n",
    "            bigram_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4794"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the original tweets to the possible word paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in nltk_tweets: \n",
    "    bigram_paths.append(list(bigrams([token for token in tweets])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'At', u'least'),\n",
       " (u'least', u'two'),\n",
       " (u'two', u'dead'),\n",
       " (u'dead', u'after'),\n",
       " (u'after', u'Nepal'),\n",
       " (u'Nepal', u'quake')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_paths[4795]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I want to turn the tweets from bigrams to actual sentences (or at least, lists of unicode). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_list(bigram_path):\n",
    "    '''\n",
    "    This method takes a bigram path (eg. [(u'hello', u'world'), (u'world', u'!')]) and returns \n",
    "    a list of unicode (eg [u'hello', u'world', u'!')\n",
    "    '''\n",
    "    unicode_list = []\n",
    "    unicode_list.append(bigram_path[0][0])\n",
    "    unicode_list.append(bigram_path[0][1])\n",
    "    \n",
    "    for bigram in bigram_path[1:]:\n",
    "        unicode_list.append(bigram[1])\n",
    "    \n",
    "    return unicode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Nepal',\n",
       " u'quake',\n",
       " u'-',\n",
       " u'Buildings',\n",
       " u'collapse',\n",
       " u'in',\n",
       " u'Delhi',\n",
       " u'for',\n",
       " u'queries',\n",
       " u'regarding',\n",
       " u'tragic',\n",
       " u'Earthquake',\n",
       " u'Rocks',\n",
       " u'Nepal',\n",
       " u'victims']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_list(bigram_paths[4033])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4855/4855 [00:00<00:00, 151368.83it/s]\n"
     ]
    }
   ],
   "source": [
    "word_paths = []\n",
    "for path in tqdm(bigram_paths): \n",
    "    word_paths.append(make_list(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given all these paths, I want to find the best ones. \n",
    "\n",
    "## COntent Words Based ABstractive Summarization (COWABS) \n",
    "\n",
    "One of the constraints in COWABS requires me to make a trigram language model of all the tweets in my word path. This essentially determines the probability of what the next word in the word path will be, using the common sequences of all the words in **some corpus**. \n",
    "\n",
    "Basically, my trigram language model will look at sequences of 3 words in the tweet paths, and ask 'does this sequence of three words occur commonly in a corpus of normal readable english? If so, its probably a readable sequence (and has a high probability of occuring in a normal english sentence). \n",
    "\n",
    "I now need to create this model, using a corpus of english. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Trigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per [Rudra et al](http://dl.acm.org/citation.cfm?id=2914600) (different paper from the last notebook), I want to maximize\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} LQ(i)\\cdot I(i) \\cdot x_{i} + \\sum_{j=1}^{m} y_{j}\n",
    "\\end{equation}\n",
    "where x are the paths chosen, y are the content words chosen, I(i) describes the Informativeness of some word path i and LQ(i) describes the Linguistic Quality Score of some word path i. \n",
    "\n",
    "I will need to define the Informativeness and Linguistic Quality Scores quantitatively: \n",
    "\n",
    "**Informativeness** is the cosine distance between each word-path and the mean tf-idf vector. \n",
    "\n",
    "**Linguistic Quality Score** assigns probabilities to the occurences of words, with more probable words getting higher scores, such that \n",
    "\\begin{equation}\n",
    "LQ(s_{i}) = \\frac{1}{(1 - ll(w_1, w_2, ... , w_q)}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "ll(w_{1}, w_{2}, ... , w_{q}) = \\frac{1}{L} log_{2} \\prod_{t=3}^q P(w_{t}|w_{t-1}w_{t-2})\n",
    "\\end{equation}\n",
    "The point of this constraint is to weight more probably sequences of words more highly, therefore favouring more 'realistic' sentences. \n",
    "\n",
    "As before, the word paths will be subject to the same constraints as the tweets; all content words in a word path must be selected, and if a content word is selected, so must a word path containing it. \n",
    "\n",
    "I'm going to start by defining some methods, which will make all these equations easier to define. \n",
    "\n",
    "Starting with informativeness, which is quite a bit easier to define: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def informativeness(word_path):\n",
    "    '''\n",
    "    This method returns the cosine difference between\n",
    "    a tweet path and the mean of the tf-idf term matrix\n",
    "    \n",
    "    Input = word path (as a unicode list)\n",
    "    Ouptut = cosine difference (scalar value)\n",
    "    '''\n",
    "    tfidf_mean = np.mean(term_matrix, axis = 0)\n",
    "    \n",
    "    # First, I need to construct the tf-idf vector\n",
    "    tfidf_path = np.zeros(len(tfidf_mean))\n",
    "    \n",
    "    for word in word_path: \n",
    "        word_idx = vocab_to_idx[word]\n",
    "        tfidf_path[word_idx] = np.max(term_matrix[:,word_idx])\n",
    "   \n",
    "    cosine_difference = cosine(tfidf_mean, tfidf_path)\n",
    "    return cosine_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64559910081477889"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informativeness(word_paths[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linguistic quality score, I am going to be using [kenlm](http://kheafield.com/code/kenlm/) (specifically its python implementation), which uses the trigram language model I have created above to calculate the linguistic quality for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kenlm_model = kenlm.Model('coca.arpa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the kenlm model takes as input a string, not a list of unicode, so I need to turn the word path into a string sentence before I can pass it to the kenlm model to get its score. \n",
    "\n",
    "Since this method depends on the summary length, I will define the summary length `L` here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can define the method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linguistic_quality(word_path):\n",
    "    '''\n",
    "    This method takes a word path, and returns a linguistic quality score \n",
    "    '''\n",
    "    path_string = str(\" \").join([token.encode('ascii', 'ignore') for token in word_path])\n",
    "    \n",
    "    ll_score = math.log(10**kenlm_model.score(path_string, bos = True, eos = True), 2)/L\n",
    "    \n",
    "    # scale by 100 so this meaningfully impacts the Integer Programming Solution\n",
    "    return 1/(1-ll_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I may be picking the best of a bad bunch here. \n",
    "\n",
    "The constraints are actually the same as for the COWTS model:\n",
    "\n",
    "1. \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}\n",
    "I want the total length of all the selected word paths to be less than some value L, which will be the length of my summary, L. I can vary L depending on how long I want my summary to be. \n",
    "\n",
    "2. \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}\n",
    "If I pick some content word $y_{j}$ (out of my $m$ possible content words) , then I want to have at least one path from the set of word paths which contain that content word, $T_{j}$. \n",
    "\n",
    "3. \n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}\n",
    "If I pick some path i (out of my $n$ possible paths) , then all the content words in that path $C_{i}$ are also selected. \n",
    "\n",
    "Let's begin the ILP step, once again using  [PyMathProg](http://pymprog.sourceforge.net/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymprog import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWABS') is the default model."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin('COWABS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining my first variable, x \n",
    "# This defines whether or not a word path is selected\n",
    "x = var(str('x'), len(word_paths), bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also defining the second variable, which defines\n",
    "# whether or not a content word is chosen\n",
    "y = var(str('y'), len(content_vocab), bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have defined my variables, I can define my equation to maximize: \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} LQ(i)\\cdot I(i) \\cdot x_{i} + \\sum_{j=1}^{m} y_{j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maximize(sum([linguistic_quality(word_paths[i])*informativeness(word_paths[i])*x[i] for i in range(len(x))]) + \n",
    "         sum(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can define my constraints. First, \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Maximum length of the entire tweet summary L has been defined above\n",
    "## when finding the linguistic quality score\n",
    "\n",
    "# hiding the output of this line since its a very long sum \n",
    "sum([x[i]*len(word_paths[i]) for i in range(len(x))]) <= L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for COWTS, I define two helper methods for the next two constrains. \n",
    "\n",
    "Since I don't have a term matrix, they need to be slighly rewritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_words(i):\n",
    "    '''Given a word path index i (for x[i]), this method will return the indices of the words in the \n",
    "    content_vocab[] array\n",
    "    Note: these indices are the same as for the y variable\n",
    "    '''\n",
    "    path = word_paths[i]\n",
    "    content_indices = []\n",
    "    \n",
    "    for word in path:\n",
    "        if word in content_vocab:\n",
    "            content_indices.append(content_vocab.index(word))\n",
    "    return content_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paths_with_content_words(j):\n",
    "    '''Given the index j of some content word (for content_vocab[j] or y[j])\n",
    "    this method will return the indices of all tweets which contain this content word\n",
    "    '''\n",
    "    content_word = content_vocab[j]\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    for i in range(len(word_paths)):\n",
    "        if content_word in word_paths[i]:\n",
    "            indices.append(i)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now define the second constraint: \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(y)):\n",
    "    sum([x[i] for i in paths_with_content_words(j)])>= y[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the third constraint:\n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    sum(y[j] for j in content_words(i)) >= len(content_words(i))*x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.) \\nThe MIP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_x =  [value.primal for value in x]\n",
    "result_y = [value.primal for value in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWABS') is not the default model."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_paths = np.nonzero(result_x)\n",
    "chosen_words = np.nonzero(result_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      ": LATEST Nepal's Kantipur TV shows at Ahmedabad from Kathmandu RestlessEarth GeographyNow\n",
      "--------------\n",
      "MEA opens 24hr Control Room in Nepal 20 00 29 UTC quake\n",
      "--------------\n",
      ": EarthquakeInNepal Helpline Numbers of Lamjung , Purvanchal & Kochi too !\n",
      "--------------\n",
      ": Warning India Bangladesh Pakistan Afganistan Bhutan Nepal Earthquake Rocks Nepal BBC\n",
      "--------------\n",
      "Dharahara also called Bhimsen Tower , 2 at 09:30 UTC , 9851135141\n",
      "--------------\n",
      "( Houston _0998 ) Avalanche Sweeps Everest in Nepal - New York Times\n",
      "--------------\n",
      "Kathmandu's Darbar Square after 7.9 magnitude in Tibet Nepalquake Embedded image permalink\n",
      "--------------\n",
      "5.00 earthquake Kathmandu Ambulance and 11 2301 2113 011 2301 4104 011 2301 7905\n",
      "--------------\n",
      "Update on 4/25 / 2015 06:37 UTC , Katmandu - Fox News\n",
      "--------------\n",
      ": 09771 4261945 / 15 @ 9:30 : Nepal AssociatedPress Associated Press news\n",
      "--------------\n",
      "Bravo Google You are faster than 300 people within 100km . 9 - France 24\n",
      "--------------\n",
      ": Patan Durbar Square after 7.7 quake : 079-2325190 0/902 / 9779851135 141\n"
     ]
    }
   ],
   "source": [
    "for i in chosen_paths[0]:\n",
    "    print ('--------------')\n",
    "    print str(\" \").join([token.encode('ascii', 'ignore') for token in word_paths[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
