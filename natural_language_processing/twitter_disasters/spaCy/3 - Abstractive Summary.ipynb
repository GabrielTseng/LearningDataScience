{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "# warning: the below makes unicode the default\n",
    "from __future__ import unicode_literals \n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import nltk\n",
    "import inspect\n",
    "\n",
    "from textacy.vsm import Vectorizer\n",
    "import textacy.vsm\n",
    "\n",
    "import spacy\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "import kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cowts_tweets = pd.read_pickle('cowts_tweets.pkl')\n",
    "term_matrix = np.load('term_matrix.npy')\n",
    "vocab_to_idx = np.load('vocab_to_idx.npy').item()\n",
    "tweet_indices = np.load('tweet_indices.npy')\n",
    "\n",
    "content_vocab = list(np.load('content_vocab.npy'))\n",
    "tfidf_dict = np.load('tfidf_dict.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing it using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spacy_tweets = []\n",
    "\n",
    "for doc in nlp.pipe(cowts_tweets[0], n_threads = -1):\n",
    "    spacy_tweets.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these 59 tweets, which consist of a 1000 word summary of the dataset, I want to generate a paragraph summary from the words. I am going to do this in two steps: \n",
    "\n",
    "1. First, I am going to generate a bigram of the words. This will map all of the words in all of the tweets to all the other words they are connected to. \n",
    "\n",
    "2. Then, I am going to use the bigram to find an optimal 'text path', which I will solve using Integer Linear Programming. \n",
    "\n",
    "Some of the tweets are single word tweets (eg. '`Awful`'). These will not contribute anything to the word graph, so I remove them from this list of chosen tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_tweets = [tweet for tweet in spacy_tweets if len(tweet) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a word graph\n",
    "\n",
    "I want to make a bigram; each node of a bigram is two adjacent words in a tweet. I can then generate sentences by traversing paths (which lead to the same word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for a single tweet, a bigram would look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(:, LATEST),\n",
       " (LATEST, Nepal),\n",
       " (Nepal, 's),\n",
       " ('s, Kantipur),\n",
       " (Kantipur, TV),\n",
       " (TV, shows),\n",
       " (shows, at),\n",
       " (at, least),\n",
       " (least, 21),\n",
       " (21, bodies),\n",
       " (bodies, lined),\n",
       " (lined, up),\n",
       " (up, on),\n",
       " (on, ground),\n",
       " (ground, after),\n",
       " (after, 7.9),\n",
       " (7.9, earthquake),\n",
       " (earthquake, \n",
       "   )]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(bigrams(spacy_tweets[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now construct this for all the tweets, simply by adding their respective bigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_bigrams = [list(bigrams([token.lemma_ for token in tweets])) for tweets in spacy_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I take the the starting and end nodes of the bigrams, so that I can generate word paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starting_nodes = [single_bigram[0] for single_bigram in all_bigrams]\n",
    "end_nodes = [single_bigram[-1] for single_bigram in all_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_bigrams = [node for single_bigram in all_bigrams for node in single_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_bigrams = list(set(all_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bigrams themselves are not super useful; what I want to do is compile a list of all the word paths through the bigram. I can do this by using the starts of the tweets as the 'beginnings', and the ends as 'ends'. \n",
    "\n",
    "In order to limit the number of word paths, I will limit the path length to between 10 and 15 paths. I'm going to implement a breadth first search to find these word paths.\n",
    "\n",
    "The first step is to take my bigram list and turn it into a dictionary; this will make it easier to find the paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_bigram_graph(all_bigrams, start_node):\n",
    "    bigram = all_bigrams[:]\n",
    "    \n",
    "    '''\n",
    "    Given a bigram, with a defined start node and defined end nodes, this method\n",
    "    returns a dictionary which serves as a graph for that bigram \n",
    "    '''\n",
    "    def find_children(bigram, node):\n",
    "        '''\n",
    "        Given a node, this method finds all its children \n",
    "        '''\n",
    "        second_word = node[1]\n",
    "        \n",
    "        children = [node for node in bigram if node[0] == second_word]\n",
    "        \n",
    "        return children\n",
    "   \n",
    "    bigram_graph = {}\n",
    "    # start by adding the start node\n",
    "    bigram_graph[start_node] = find_children(all_bigrams, start_node)\n",
    "    bigram.remove(start_node)\n",
    "    \n",
    "    nodes_to_check = []\n",
    "    for i in find_children(bigram, start_node):\n",
    "        nodes_to_check.append(i)\n",
    "        \n",
    "    while nodes_to_check: \n",
    "        node = nodes_to_check.pop()\n",
    "        if node in bigram: \n",
    "            bigram_graph[node] = find_children(bigram, node)\n",
    "            bigram.remove(node)\n",
    "            for i in find_children(bigram, node):\n",
    "                nodes_to_check.append(i)\n",
    "    return bigram_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_graph = make_bigram_graph(all_bigrams, starting_nodes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "819"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have this dictionary, I am going to implement a breadth first search to find all possible paths between a start node and an end node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def breadth_first_search(bigram_graph, start_node, end_node):\n",
    "    '''\n",
    "    This method takes as input a graph, a start node and an end node\n",
    "    and returns all paths which have a length between 10 and 16\n",
    "    between the two nodes.\n",
    "    '''\n",
    "    graph_to_manipulate = dict(bigram_graph)\n",
    "    \n",
    "    queue = []\n",
    "    paths_to_return = []\n",
    "    queue.append([start_node])\n",
    "    \n",
    "    while queue:\n",
    "        # get the first path from the queue\n",
    "        path = queue.pop(0)\n",
    "        # get the last node from the path\n",
    "        node = path[-1]\n",
    "        # path found\n",
    "        if node == end_node:\n",
    "            if (len(path) < 16) and (len(path) > 10): #limit path length \n",
    "                paths_to_return.append(path)\n",
    "        # enumerate all adjacent nodes, construct a new path and push it into the queue\n",
    "        for adjacent in graph_to_manipulate.get(node, []):\n",
    "            new_path = list(path)\n",
    "            new_path.append(adjacent)\n",
    "            queue.append(new_path)\n",
    "        if node in graph_to_manipulate: \n",
    "            del graph_to_manipulate[node] # prevents circular references\n",
    "\n",
    "    return paths_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = breadth_first_search(bigram_graph, starting_nodes[1], end_nodes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the first path this produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'prayer', u'for'),\n",
       "  (u'for', u'the'),\n",
       "  (u'the', u'parts'),\n",
       "  (u'parts', u'of'),\n",
       "  (u'of', u'lamjung'),\n",
       "  (u'lamjung', u','),\n",
       "  (u',', u'2'),\n",
       "  (u'2', u'in'),\n",
       "  (u'in', u'solu'),\n",
       "  (u'solu', u'dist'),\n",
       "  (u'dist', u'accord'),\n",
       "  (u'accord', u'\\x89\\xfb'),\n",
       "  (u'\\x89\\xfb', u'_')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonsensical, but this is a word path. Success! \n",
    "\n",
    "Now, I just need to repeat this exercise for every starting node, mapping to every end node, to collect a 'total word path' document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:26<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "bigram_paths = []\n",
    "\n",
    "for single_start_node in tqdm(starting_nodes): \n",
    "    bigram_graph = make_bigram_graph(all_bigrams, single_start_node)\n",
    "    for single_end_node in end_nodes:\n",
    "        possible_paths = breadth_first_search(bigram_graph, single_start_node, single_end_node)\n",
    "        for path in possible_paths: \n",
    "            bigram_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4032"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the original tweets to the possible word paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in spacy_tweets: \n",
    "    bigram_paths.append(list(bigrams([token.lemma_ for token in tweets])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u':', u'.@su4ita'),\n",
       " (u'.@su4ita', u'for'),\n",
       " (u'for', u'blood'),\n",
       " (u'blood', u'requirement'),\n",
       " (u'requirement', u'in'),\n",
       " (u'in', u'kathmandu'),\n",
       " (u'kathmandu', u'contact'),\n",
       " (u'contact', u'mr.'),\n",
       " (u'mr.', u'adhikari'),\n",
       " (u'adhikari', u'00977'),\n",
       " (u'00977', u'-'),\n",
       " (u'-', u'9862005225')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_paths[4033]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I want to turn the tweets from bigrams to actual sentences (or at least, lists of unicode). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_list(bigram_path):\n",
    "    '''\n",
    "    This method takes a bigram path (eg. [(u'hello', u'world'), (u'world', u'!')]) and returns \n",
    "    a list of unicode (eg [u'hello', u'world', u'!')\n",
    "    '''\n",
    "    unicode_list = []\n",
    "    unicode_list.append(bigram_path[0][0])\n",
    "    unicode_list.append(bigram_path[0][1])\n",
    "    \n",
    "    for bigram in bigram_path[1:]:\n",
    "        unicode_list.append(bigram[1])\n",
    "    \n",
    "    return unicode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u':',\n",
       " u'.@su4ita',\n",
       " u'for',\n",
       " u'blood',\n",
       " u'requirement',\n",
       " u'in',\n",
       " u'kathmandu',\n",
       " u'contact',\n",
       " u'mr.',\n",
       " u'adhikari',\n",
       " u'00977',\n",
       " u'-',\n",
       " u'9862005225']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_list(bigram_paths[4033])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4092/4092 [00:00<00:00, 158138.54it/s]\n"
     ]
    }
   ],
   "source": [
    "word_paths = []\n",
    "for path in tqdm(bigram_paths): \n",
    "    word_paths.append(make_list(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given all these paths, I want to find the best ones. \n",
    "\n",
    "## COntent Words Based ABstractive Summarization (COWABS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per [Rudra et al](http://dl.acm.org/citation.cfm?id=2914600) (different paper from the last notebook), I want to maximize\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} LQ(i)\\cdot I(i) \\cdot x_{i} + \\sum_{j=1}^{m} y_{j}\n",
    "\\end{equation}\n",
    "where x are the paths chosen, y are the content words chosen, I(i) describes the Informativeness of some word path i and LQ(i) describes the Linguistic Quality Score of some word path i. \n",
    "\n",
    "I will need to define the Informativeness and Linguistic Quality Scores quantitatively: \n",
    "\n",
    "**Informativeness** is the cosine distance between each word-path and the mean tf-idf vector. \n",
    "\n",
    "**Linguistic Quality Score** assigns probabilities to the occurences of words, with more probable words getting higher scores, such that \n",
    "\\begin{equation}\n",
    "LQ(s_{i}) = \\frac{1}{(1 - ll(w_1, w_2, ... , w_q)}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "ll(w_{1}, w_{2}, ... , w_{q}) = \\frac{1}{L} log_{2} \\prod_{t=3}^q P(w_{t}|w_{t-1}w_{t-2})\n",
    "\\end{equation}\n",
    "The point of this constraint is to weight more probably sequences of words more highly, therefore favouring more 'realistic' sentences. \n",
    "\n",
    "As before, the word paths will be subject to the same constraints as the tweets; all content words in a word path must be selected, and if a content word is selected, so must a word path containing it. \n",
    "\n",
    "I'm going to start by defining some methods, which will make all these equations easier to define. \n",
    "\n",
    "Starting with informativeness, which is quite a bit easier to define: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def informativeness(word_path):\n",
    "    '''\n",
    "    This method returns the cosine difference between\n",
    "    a tweet path and the mean of the tf-idf term matrix\n",
    "    \n",
    "    Input = word path (as a unicode list)\n",
    "    Ouptut = cosine difference (scalar value)\n",
    "    '''\n",
    "    tfidf_mean = np.mean(term_matrix, axis = 0)\n",
    "    \n",
    "    # First, I need to construct the tf-idf vector\n",
    "    tfidf_path = np.zeros(len(tfidf_mean))\n",
    "    \n",
    "    for word in word_path: \n",
    "        word_idx = vocab_to_idx[word]\n",
    "        tfidf_path[word_idx] = np.max(term_matrix[:,word_idx])\n",
    "   \n",
    "    cosine_difference = cosine(tfidf_mean, tfidf_path)\n",
    "    return cosine_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8106635475960231"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informativeness(word_paths[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linguistic quality score, I am going to be using [kenlm](http://kheafield.com/code/kenlm/) (specifically its python implementation), which actually calculates this quality for me. \n",
    "\n",
    "Like SpaCy, the model must be defined: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kenlm_model = kenlm.Model('coca.arpa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the kenlm model takes as input a string, not a list of unicode, so I need to turn the word path into a string sentence before I can pass it to the kenlm model to get its score. \n",
    "\n",
    "Since this method depends on the summary length, I will define the summary length `L` here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linguistic_quality(word_path):\n",
    "    '''\n",
    "    This method takes a word path, and returns a linguistic quality score \n",
    "    '''\n",
    "    path_string = str(\" \").join([token.encode('ascii', 'ignore') for token in word_path])\n",
    "    \n",
    "    ll_score = math.log(10**kenlm_model.score(path_string, bos = True, eos = True), 2)/L\n",
    "    \n",
    "    return (1/(1-ll_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I may be picking the best of a bad bunch here. \n",
    "\n",
    "The constraints are actually the same as for the COWTS model:\n",
    "\n",
    "1. \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}\n",
    "I want the total length of all the selected word paths to be less than some value L, which will be the length of my summary, L. I can vary L depending on how long I want my summary to be. \n",
    "\n",
    "2. \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}\n",
    "If I pick some content word $y_{j}$ (out of my $m$ possible content words) , then I want to have at least one path from the set of word paths which contain that content word, $T_{j}$. \n",
    "\n",
    "3. \n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}\n",
    "If I pick some path i (out of my $n$ possible paths) , then all the content words in that path $C_{i}$ are also selected. \n",
    "\n",
    "Let's begin the ILP step, once again using  [PyMathProg](http://pymprog.sourceforge.net/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymprog import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWABS') is the default model."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin('COWABS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining my first variable, x \n",
    "# This defines whether or not a word path is selected\n",
    "x = var(str('x'), len(word_paths), bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also defining the second variable, which defines\n",
    "# whether or not a content word is chosen\n",
    "y = var(str('y'), len(content_vocab), bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have defined my variables, I can define my equation to maximize: \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} LQ(i)\\cdot I(i) \\cdot x_{i} + \\sum_{j=1}^{m} y_{j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maximize(sum([linguistic_quality(word_paths[i])*informativeness(word_paths[i])*x[i] for i in range(len(x))]) + \n",
    "         sum(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can define my constraints. First, \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hiding the output of this line since its a very long sum \n",
    "sum([x[i]*len(word_paths[i]) for i in range(len(x))]) <= L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for COWTS, I define two helper methods for the next two constrains. \n",
    "\n",
    "Since I don't have a term matrix, they need to be slighly rewritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_words(i):\n",
    "    '''Given a word path index i (for x[i]), this method will return the indices of the words in the \n",
    "    content_vocab[] array\n",
    "    Note: these indices are the same as for the y variable\n",
    "    '''\n",
    "    path = word_paths[i]\n",
    "    content_indices = []\n",
    "    \n",
    "    for word in path:\n",
    "        if word in content_vocab:\n",
    "            content_indices.append(content_vocab.index(word))\n",
    "    return content_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paths_with_content_words(j):\n",
    "    '''Given the index j of some content word (for content_vocab[j] or y[j])\n",
    "    this method will return the indices of all tweets which contain this content word\n",
    "    '''\n",
    "    content_word = content_vocab[j]\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    for i in range(len(word_paths)):\n",
    "        if content_word in word_paths[i]:\n",
    "            indices.append(i)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now define the second constraint: \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(y)):\n",
    "    sum([x[i] for i in paths_with_content_words(j)])>= y[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the third constraint:\n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    sum(y[j] for j in content_words(i)) >= len(content_words(i))*x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.) \\nThe MIP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_x =  [value.primal for value in x]\n",
    "result_y = [value.primal for value in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWABS') is not the default model."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_paths = np.nonzero(result_x)\n",
    "chosen_words = np.nonzero(result_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "avalanche sweeps everest base camp , 34 minute of major earthquake \r",
      " \n",
      "--------------\n",
      ": mea control room no for nepal 25/04/2015 07:13 utc , april 25,nepalquake kathmanduquake\n",
      "--------------\n",
      "magnitude-7.9 quake hits nepal nepalquake nepalearthquake \r",
      " high alert after 7.9 magnitude earthquake perso _\n",
      "--------------\n",
      "earthquake m7.5 strike 89 km nw of 4.5 + 91 11 2301 7905\n",
      "--------------\n",
      "thr r safe . apr 25 14:14 at 7.7 richter scale , via\n",
      "--------------\n",
      "sad day for the last 1 hour(s ) .   associatedpress associated press news\n",
      "--------------\n",
      ": whole himalayan region be up and lalitpur make kathmandu 's 19th century nine - witness\n",
      "--------------\n",
      ": 09771 4261945/ 4261790 emergency helpline number in 80 year - typical indian\n",
      "--------------\n",
      ": patan durbar square   afganistan bhutan emb \r",
      " pm \r",
      " 9779851135141\n",
      "--------------\n",
      "building collapse , 400 people kill in kathmandu-+977 98511 07021 , 9851135141\n",
      "--------------\n",
      "historic dharara tower \r",
      "\r",
      " nepal n north east . kathmandu contact mr. adhikari 00977 - cnn\n"
     ]
    }
   ],
   "source": [
    "for i in chosen_paths[0]:\n",
    "    print ('--------------')\n",
    "    print str(\" \").join([token.encode('ascii', 'ignore') for token in word_paths[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
