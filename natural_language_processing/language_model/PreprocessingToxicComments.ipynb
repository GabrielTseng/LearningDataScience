{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tokenizes the `Toxic comments` datasets.\n",
    "\n",
    "The tokenizing uses multiprocessing, and returns a list of ints, and (optionally) a dictionary which turns those ints into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from lm.preprocess import ToxicTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = Path('toxic_data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the path input is a list; if multiple paths are in the list, they will be concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = Path('word2int.pickle')\n",
    "with dict_path.open('rb') as file:\n",
    "    d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 159571 articles\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer = ToxicTokenizer([train_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`preprocess` will accept a dictionary as input; in this case, the dictionary created when training the language model, using the wikitext dataset, will be used.\n",
    "\n",
    "This will allow that model to be finetuned for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 comments\n",
      "Processed 20000 comments\n",
      "Processed 30000 comments\n",
      "Processed 40000 comments\n",
      "Processed 50000 comments\n",
      "Processed 60000 comments\n",
      "Processed 70000 comments\n",
      "Processed 80000 comments\n",
      "Processed 90000 comments\n",
      "Processed 100000 comments\n",
      "Processed 110000 comments\n",
      "Processed 120000 comments\n",
      "Processed 130000 comments\n",
      "Processed 140000 comments\n",
      "Processed 150000 comments\n",
      "Tokenized articles!\n",
      "Coverage is 91.46685216843248 %\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_ints = train_tokenizer.preprocess(word2int=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, header2index = train_tokenizer.get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test / Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = Path('toxic_data/test.csv')\n",
    "test_labels = Path('toxic_data/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 153164 articles\n"
     ]
    }
   ],
   "source": [
    "valtest_tokenizer = ToxicTokenizer([test_comments], label_filepaths=[test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 comments\n",
      "Processed 20000 comments\n",
      "Processed 30000 comments\n",
      "Processed 40000 comments\n",
      "Processed 50000 comments\n",
      "Processed 60000 comments\n",
      "Processed 70000 comments\n",
      "Processed 80000 comments\n",
      "Processed 90000 comments\n",
      "Processed 100000 comments\n",
      "Processed 110000 comments\n",
      "Processed 120000 comments\n",
      "Processed 130000 comments\n",
      "Processed 140000 comments\n",
      "Processed 150000 comments\n",
      "Tokenized articles!\n",
      "Coverage is 90.07447419439735 %\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_ints = valtest_tokenizer.preprocess(word2int=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, test_header2index = valtest_tokenizer.get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('toxic_train_int_tokens.npy', np.asarray(tokenized_train_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('toxic_test_int_tokens.npy', np.asarray(tokenized_test_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_header2index == header2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = Path('toxic_header2index.pickle')\n",
    "with dict_path.open(mode='wb') as file:\n",
    "    pickle.dump(header2index, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_labels.npy', np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_labels', np.asarray(test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
