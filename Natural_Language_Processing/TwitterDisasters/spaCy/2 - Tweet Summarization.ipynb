{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "import inspect\n",
    "\n",
    "from textacy.vsm import Vectorizer\n",
    "import textacy.vsm\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweet_ids/2015_Nepal_Earthquake_en/stripped_filled_tweets.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>infrastructure_and_utilities_damage</td>\n",
       "      <td>'591902695822331904'</td>\n",
       "      <td>RT @DailySabah: #LATEST #Nepal's Kantipur TV s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>'591902695943843840'</td>\n",
       "      <td>RT @iamsrk: May Allah look after all. Here r t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>'591902696371724288'</td>\n",
       "      <td>RT @RT_com: LATEST: 108 killed in 7.9-magnitud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sympathy_and_emotional_support</td>\n",
       "      <td>'591902696375877632'</td>\n",
       "      <td>RT @Edourdoo: Shocking picture of the earthqua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>sympathy_and_emotional_support</td>\n",
       "      <td>'591902696895950848'</td>\n",
       "      <td>Indian Air Force is ready to help the people o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                label              tweet_id  \\\n",
       "0          1  infrastructure_and_utilities_damage  '591902695822331904'   \n",
       "1          2               injured_or_dead_people  '591902695943843840'   \n",
       "2          3      missing_trapped_or_found_people  '591902696371724288'   \n",
       "3          4       sympathy_and_emotional_support  '591902696375877632'   \n",
       "4          5       sympathy_and_emotional_support  '591902696895950848'   \n",
       "\n",
       "                                         tweet_texts  \n",
       "0  RT @DailySabah: #LATEST #Nepal's Kantipur TV s...  \n",
       "1  RT @iamsrk: May Allah look after all. Here r t...  \n",
       "2  RT @RT_com: LATEST: 108 killed in 7.9-magnitud...  \n",
       "3  RT @Edourdoo: Shocking picture of the earthqua...  \n",
       "4  Indian Air Force is ready to help the people o...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For my tweets to be informative, there are a few terms I can immediately remove. For instance, any urls won't be useful to the rescue teams. Equally, any '@...' are just calling another twitter handle, and are equally not useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing URLS\n",
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'http\\S+', u'', x))   \n",
    "\n",
    "# removing @... \n",
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'(\\s)@\\w+', u'', x))\n",
    "\n",
    "# removing hashtags\n",
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'#', u'', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT: LATEST Nepal's Kantipur TV shows at least ...\n",
       "1    RT: May Allah look after all. Here r the emerg...\n",
       "2    RT: LATEST: 108 killed in 7.9-magnitude Nepal ...\n",
       "3    RT: Shocking picture of the earthquake in Nepa...\n",
       "4    Indian Air Force is ready to help the people o...\n",
       "Name: tweet_texts, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.tweet_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alot of `u'RT'` terms in the tweet texts. Since this isn't a word, SpaCy doesn't know how to handle them. Since these add nothing to the content of a tweet, I'm just going to get rid of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: x.replace(u'RT', u''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spacy_tweets = []\n",
    "\n",
    "for doc in nlp.pipe(tweets.tweet_texts, n_threads = -1):\n",
    "    spacy_tweets.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEA opens 24 hour Control Room for queries regarding the Nepal Earthquake.åÊ\n",
       "Numbers:\n",
       "+91 11 2301 2113\n",
       "+91 11 2301 4104\n",
       "+91 11 2301 7905"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tweets[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all tweets are equally useful. Some just contain prayers, such as\n",
    "\n",
    "`Hope it doesn't rain. #Nepal`\n",
    "\n",
    "whereas others are dense with useful information: \n",
    "\n",
    "`2 Dead, 100 Injured in Bangladesh From Nepal Quake`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I decide which parts of these tweets are most useful? One way to do it is to measure the term frequency-inverse document frequency (tf-idf) of each of the words in the corpus of tweets. This metric measures how important a word is in a corpus of tweets. \n",
    "\n",
    "## Getting the tf-idf values of content words. \n",
    "\n",
    "I can do a preliminary 'cleanup', by keeping only 'content words'. These are defined as : Numerals, Nouns and Verbs. Conveniantly, SpaCy has already organised this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ": BREAKING: At least 114 killed in Nepal earthquake: home ministry - Read  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tweets[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I care most about tokens which are entities, and numbers. The other tokens have too much noise, so let's focus on these two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_words = [u'earthquake', u'killed', u'injured', u'stranded', u'wounded', u'hurt', u'helpless', u'wrecked', u'nepal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_entities = [u'NORP', u'FACILITY', u'ORG', u'GPE', u'LOC', u'EVENT', u'DATE', u'TIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2337/2337 [00:00<00:00, 24086.83it/s]\n"
     ]
    }
   ],
   "source": [
    "content_tweets = []\n",
    "for single_tweet in tqdm(spacy_tweets):\n",
    "    single_tweet_content = []\n",
    "    for token in single_tweet: \n",
    "        if ((token.ent_type_ in useful_entities)  \n",
    "            or (token.pos_ == u'NUM') \n",
    "            or (token.lower_ in main_words)):\n",
    "            single_tweet_content.append(token)\n",
    "    content_tweets.append(single_tweet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_tweet \n",
      "MEA opens 24 hour Control Room for queries regarding the Nepal Earthquake.åÊ\r",
      "Numbers:\r",
      "+91 11 2301 2113\r",
      "+91 11 2301 4104\r",
      "+91 11 2301 7905\n",
      "\n",
      "original_tweet\n",
      "['MEA', 'opens', '24', 'hour', 'Control', 'Room', 'for', 'queries', 'regarding', 'the', 'Nepal', 'Earthquake.\\xc3\\xa5\\xc3\\x8a', '\\r', 'Numbers', ':', '\\r', '+', '91', '11', '2301', '2113', '\\r', '+', '91', '11', '2301', '4104', '\\r', '+', '91', '11', '2301', '7905']\n",
      "\n",
      "content_tweet\n",
      "[MEA, 24, hour, Control, Room, Nepal, 91, 11, 2301, 2113, 91, 11, 2301, 4104, 91, 11, 2301, 7905]\n"
     ]
    }
   ],
   "source": [
    "tweet_num = 200\n",
    "print (\"original_tweet \\n\" + str(spacy_tweets[tweet_num]) \n",
    "       + \"\\n\\noriginal_tweet\\n\" + str([str(x) for x in spacy_tweets[tweet_num]])\n",
    "       + \"\\n\\ncontent_tweet\\n\" + str(content_tweets[tweet_num])\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this has already gone some way to (crudely) isolating the interesting parts of a tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, SpaCy doesn't calculate tf-idf score automatically. There IS a library which can do this: [textacy](https://textacy.readthedocs.io/en/latest/index.html). Note: textacy is built on SpaCy.\n",
    "\n",
    "I care about the tf-idf scores of the entire tweet, so will find the tf-idf score across the entire corpus of original tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(weighting = 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the tf-idf score of all the tokens in the tweets, I can use `fit_transform()`. \n",
    "\n",
    "Note: I am using the `lemma_` attribute of each token, because tokens contain information about the documents. This means that 'Nepal' in the 100th tweet will have a different **token** from 'Nepal' in the 200th tweet, but the same `lemma__` attribute. This is what I want to compare - I don't want hundreds of 'Nepal' columns in my term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_matrix = vectorizer.fit_transform([tok.lemma_ for tok in doc] for doc in spacy_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is a term-document matrix. What this means is that on top of having the tf-idf values, each row is a document (and each column is a word). \n",
    "\n",
    "If the tweet in row `i` contains the column in row `j`, then the element `matrix[i][j]` will contain the tf-idf value. If the tweet *doesn't* contan the word, the matrix value will be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_matrix = term_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2337, 2184)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My ultimate goal is to create a dictionary, which maps from the tokens in the content tweets to some tf-idf score. To do this, I need to find out which tokens are at what columns in the term matrix. \n",
    "\n",
    "The vectorizer object has a dictionary, which maps token.lemma_ to its column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "himalayan 722\n",
      "himaû 629\n",
      "hindan 1536\n",
      "hindu 1486\n",
      "hindus 1808\n",
      "historic 506\n",
      "historical 910\n",
      "history 628\n",
      "hit 94\n",
      "hits 1244\n",
      "hn 1283\n",
      "hold 1144\n",
      "hom 803\n",
      "home 264\n",
      "hop 1522\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(vectorizer.vocabulary)[1000:1015]:\n",
    "    print key, vectorizer.vocabulary[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And each column (word) has a unique tf-idf value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can therefore map the value of the content tokens to their tf-idf, using the `vectorizer.vocabulary` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'earthquake', 17, 2.9568443994226628)\n",
      "(u'indian', 48, 4.2797143275538065)\n",
      "(u'kathmandu', 96, 5.2647355022756184)\n",
      "(u'977', 255, 9.9007773045233876)\n",
      "(u'98511', 256, 11.522637736956046)\n",
      "(u'07021', 884, 6.0490009409298038)\n",
      "(u'977', 255, 9.9007773045233876)\n",
      "(u'98511', 256, 11.522637736956046)\n",
      "(u'35141\"\\x89\\xfb', 885, 7.3707567809121235)\n"
     ]
    }
   ],
   "source": [
    "for token in content_tweets[500]:\n",
    "    print (token.lemma_, vectorizer.vocabulary[token.lemma_], np.max(np_matrix[:,vectorizer.vocabulary[token.lemma_]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_dict = {}\n",
    "content_vocab = []\n",
    "for tweet in content_tweets: \n",
    "    for token in tweet: \n",
    "        if token.lemma_ not in tfidf_dict: \n",
    "            content_vocab.append(token.lemma_)\n",
    "            tfidf_dict[token.lemma_] = np.max(np_matrix[:,vectorizer.vocabulary[token.lemma_]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD:property -- tf-idf SCORE:5.98446241979\n",
      "WORD:pulse -- tf-idf SCORE:8.06390396147\n",
      "WORD:purvanchal -- tf-idf SCORE:7.65843885336\n",
      "WORD:quake -- tf-idf SCORE:6.40818311422\n",
      "WORD:r -- tf-idf SCORE:9.22783283128\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(tfidf_dict)[500:505]:\n",
    "    print (\"WORD:\" + str(key) + \" -- tf-idf SCORE:\" +  str(tfidf_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## COntent Word-based Tweet Summarization (COWTS) \n",
    "As per [Rudra et al](http://dl.acm.org/citation.cfm?id=2806485). \n",
    "\n",
    "I'll be using [PyMathProg](http://pymprog.sourceforge.net/index.html) as my Integer Linear Programming Solver. This is a python interface for [GLPK](https://www.gnu.org/software/glpk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymprog import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to maximize \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^n x_{i} + \\sum_{j = 1}^{m} Score(j) \\cdot y_{j}\n",
    "\\end{equation}\n",
    "Where $x_{i}$ is 1 if I include tweet i, or 0 if I don't, and where $y_{j}$ is 1 or 0 if each content word is included (and Score(j) is that word's tf-idf score). \n",
    "\n",
    "I'm going to subject this equation to the following constraints: \n",
    "\n",
    "1. \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}\n",
    "I want the total length of all the selected tweets to be less than some value L, which will be the length of my summary, L. I can vary L depending on how long I want my summary to be. \n",
    "\n",
    "2. \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}\n",
    "If I pick some content word $y_{j}$ (out of my $m$ possible content words) , then I want to have at least one tweet from the set of tweets which contain that content word, $T_{j}$. \n",
    "\n",
    "3. \n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}\n",
    "If I pick some tweet i (out of my $n$ possible tweets) , then all the content words in that tweet $C_{i}$ are also selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWTS') is the default model."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin('COWTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0 <= x[1000] <= 1 binary"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining my first variable, x \n",
    "# This defines whether or not a tweet is selected\n",
    "x = var('x', len(spacy_tweets), bool)\n",
    "\n",
    "# Check this worked\n",
    "x[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Also defining the second variable, which defines\n",
    "# whether or not a content word is chosen\n",
    "y = var('y', len(content_vocab), bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611, 0 <= y[0] <= 1 binary)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y), y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have defined my variables, I can define the equation I am maximizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maximize(sum(x) + sum([tfidf_dict[content_vocab[j]]*y[j] for j in range(len(y))]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can define my constraints. First, \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Maximum length of the entire tweet summary\n",
    "\n",
    "# Was 150 for the tweet summary, \n",
    "# But generated a 1000 word summary for CONABS\n",
    "L = 1000\n",
    "\n",
    "# hiding the output of this line since its a very long sum \n",
    "sum([x[i]*len(spacy_tweets[i]) for i in range(len(x))]) <= L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next two constraints are slightly more tricky, as I need a way to define which content words are in which tweets. \n",
    "\n",
    "However, the term matrix I defined using the vectorizer has all of this information. \n",
    "\n",
    "I'll begin by defining two helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_words(i):\n",
    "    '''Given a tweet index i (for x[i]), this method will return the indices of the words in the \n",
    "    content_vocab[] array\n",
    "    Note: these indices are the same as for the y variable\n",
    "    '''\n",
    "    tweet = spacy_tweets[i]\n",
    "    content_indices = []\n",
    "    \n",
    "    for token in tweet:\n",
    "        if token.lemma_ in content_vocab:\n",
    "            content_indices.append(content_vocab.index(token.lemma_))\n",
    "    return content_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_with_content_words(j):\n",
    "    '''Given the index j of some content word (for content_vocab[j] or y[j])\n",
    "    this method will return the indices of all tweets which contain this content word\n",
    "    '''\n",
    "    content_word = content_vocab[j]\n",
    "    \n",
    "    index_in_term_matrix = vectorizer.vocabulary[content_word]\n",
    "    \n",
    "    matrix_column = np_matrix[:, index_in_term_matrix]\n",
    "    \n",
    "    return np.nonzero(matrix_column)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now define the second constraint: \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(len(y)):\n",
    "    sum([x[i] for i in tweets_with_content_words(j)])>= y[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the third constraint:\n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    sum(y[j] for j in content_words(i)) >= len(content_words(i))*x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.) \\nThe MIP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.)'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_x =  [value.primal for value in x]\n",
    "result_y = [value.primal for value in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWTS') is not the default model."
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_tweets = np.nonzero(result_x)\n",
    "chosen_words = np.nonzero(result_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 314)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chosen_tweets[0]), len(chosen_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "MEA opens 24 hour Control Room in Delhi for queries regarding the Nepal Earthquake. \r",
      "\r",
      "011 2301 2113\r",
      "011 2301 4104\r",
      "011 2301 7905\n",
      "--------------\n",
      ": USGS reports a M5 earthquake 31km NNW of Nagarkot, Nepal on 4/25/15 @ 9:30:29 UTC  quake\n",
      "--------------\n",
      "TV: 2 dead, 100 injured in Bangladesh from Nepal quake: DHAKA, Bangladesh (AP) ÛÓ A TV r...  \n",
      "--------------\n",
      "Avalanche Sweeps Everest in Nepal; 30 Injured \n",
      "--------------\n",
      ": Earthquake helpline at the Indian Embassy in Kathmandu-+977 98511 07021, +977 98511 35141\n",
      "--------------\n",
      "earthquickinnepal shocing news earthquick in nepal may god all safe\n",
      "--------------\n",
      ": Whole Himalayan region is becoming non stable. Two yrs back Uttrakhand, then Kashmir now Nepal n north east. Even Tibet isÛ_\n",
      "--------------\n",
      "WellingtonHere Nepal's Home Ministry Says at Least 71 People Killed in the Earthquake: Nepal'...  WellingtonHere\n",
      "--------------\n",
      "Years of major earthquake-s in Nepal:\r",
      "1255\r",
      "1408\r",
      "1681\r",
      "1810\r",
      "1833\r",
      "1934\r",
      " \n",
      "--------------\n",
      "Historic Dharahara tower collapses in Kathmandu after quake | Latest News &amp; Updates at Daily... \n",
      "--------------\n",
      "7.9 magnitude earthquake rattles Nepal near Katmandu... \n"
     ]
    }
   ],
   "source": [
    "for i in chosen_tweets[0]:\n",
    "    print ('--------------')\n",
    "    print spacy_tweets[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is definitely noise amongst these tweets, but these tweets do successfully provide a good overview of the situation in Nepal.\n",
    "\n",
    "I am going to compare this to random tweets, to make sure it does perform better than 16 randomly chosen tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_tweets = np.random.choice(spacy_tweets, size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      ": Images from Everest Base Camp by UTM student Azim Afif following the quake #Kathmandu UTM camp is safe. \n",
      "--------\n",
      ": Sad 2 see this image of extensive damage due to the #earthquake in Nepal.My prayers with the victims &amp; their families \n",
      "--------\n",
      "From HN: Strong earthquake rocks Nepal, damages Kathmandu \n",
      "--------\n",
      ": OMG ! 7.9 magnitude earthquake in Nepal &amp; parts of Northern &amp; Eastern India. I pray to God that everyone is safe _Ùª _Ùª #Û_\n",
      "--------\n",
      ": MEA opens 24 hour Control Room for queries regarding the Nepal Earthquake. \r",
      "Numbers:\r",
      "+91 11 2301 2113\r",
      "+91 11 2301 4104\r",
      "+91 11Û_\n",
      "--------\n",
      ": More than 100 killed in powerful Nepal #earthquake, say government officials and police  \n",
      "--------\n",
      "@EyeshaBee I just checked out Thamel, thinking it was a distance away from Kathmandu, but it's a suburb!\n",
      "--------\n",
      ": UPDATE: Humanitarian crisis in Nepal. 100+ dead, toll may rises to 1000's. Prayers &amp; thought for Nepalese friends. \n",
      "--------\n",
      "A powerful earthquake has rocked Nepal, wrecking buildings, injuring dozens of people and causing an unknown number of deaths #Nepal\n",
      "--------\n",
      "@EyeshaBee I just checked out Thamel, thinking it was a distance away from Kathmandu, but it's a suburb!\n",
      "--------\n",
      "Raw: Powerful Earthquake Rocks Nepal  #AssociatedPress #Associated #Press #news\n"
     ]
    }
   ],
   "source": [
    "for i in random_tweets:\n",
    "    print ('--------')\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief comparison does indicate that this method is far better than random choice at providing a situational overview. \n",
    "\n",
    "It's worth noting that even a random distribution will contain a fair amount of information, because of the selective nature in which we isolated tweets; this is already a subsample which contains a higher % of relevant information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cowts_tweets = []\n",
    "for i in chosen_tweets[0]:\n",
    "    cowts_tweets.append(spacy_tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the first few tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      ": LATEST Nepal's Kantipur TV shows at least 21 bodies lined up on ground after 7.9 earthquake\r",
      " \n",
      "--------\n",
      "Prayers for the affected people across SouthAsia by the horrible earthquake!\r",
      "India Bangladesh Pakistan  Afganistan Bhutan Nepal\n",
      "--------\n",
      ": Due to bulding collaps 12 People died in Eastern Part of Nepal 5 in sunsari,5 In okhaldhunga and 2 in Solu dist according Û_\n",
      "--------\n",
      ": M7.9  - 29km ESE of Lamjung, Nepal  20 00 29 26.  Highly informative read on seismic history of HimaÛ_\n",
      "--------\n",
      "Earthquake: 2015-04-25 17:30HKT M5.0 [28.0N,85.4E] in Nepal \n",
      "--------\n",
      ": USGS reports a M5 earthquake 31km NNW of Nagarkot, Nepal on 4/25/15 @ 9:30:29 UTC  quake\n",
      "--------\n",
      "[AP] Key facts about Nepal, site of magnitude-7.9 quake \n",
      "--------\n",
      "NepalEarthquake \r",
      "PM \r",
      "spoke with Nepal prez, PM\r",
      "CM's BIHAR MP WB SIKKIM\r",
      "UP,MP sitamarhi\r",
      "Bhutan emb\r",
      "high level meeting\r",
      "NDRF dispatched\r",
      "wow!\n",
      "--------\n",
      "The BBC put the death toll even higher \n",
      "--------\n",
      ": 556 tourists to Nepal from Maharashtra safe. Indian embassy helpline number in Nepal:- 9779851107021 / \r",
      "9779851135141\n"
     ]
    }
   ],
   "source": [
    "for tweet in cowts_tweets[:10]:\n",
    "    print ('--------')\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is getting long, so I'm going to save these tweets (which I will continue using) and start a fresh notebook for the next steps. \n",
    "\n",
    "## Saving everything for a fresh notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cowts_unicode = [x.text for x in cowts_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cowts_dataframe = pd.DataFrame(cowts_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>: LATEST Nepal's Kantipur TV shows at least 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prayers for the affected people across SouthAs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: Due to bulding collaps 12 People died in Eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: M7.9  - 29km ESE of Lamjung, Nepal  20 00 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Earthquake: 2015-04-25 17:30HKT M5.0 [28.0N,85...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  : LATEST Nepal's Kantipur TV shows at least 21...\n",
       "1  Prayers for the affected people across SouthAs...\n",
       "2  : Due to bulding collaps 12 People died in Eas...\n",
       "3  : M7.9  - 29km ESE of Lamjung, Nepal  20 00 29...\n",
       "4  Earthquake: 2015-04-25 17:30HKT M5.0 [28.0N,85..."
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cowts_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving it to a pickle: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cowts_dataframe.to_pickle('cowts_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('term_matrix.npy', np_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('tweet_indices.npy', chosen_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('vocab_to_idx.npy', vectorizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('content_vocab.npy', content_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('tfidf_dict.npy', tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
