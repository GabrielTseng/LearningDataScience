{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "import inspect\n",
    "\n",
    "from textacy.vsm import Vectorizer\n",
    "import textacy.vsm\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweet_ids/2015_Nepal_Earthquake_en/stripped_filled_tweets.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>infrastructure_and_utilities_damage</td>\n",
       "      <td>'591902695822331904'</td>\n",
       "      <td>RT @DailySabah: #LATEST #Nepal's Kantipur TV s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>'591902695943843840'</td>\n",
       "      <td>RT @iamsrk: May Allah look after all. Here r t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>'591902696371724288'</td>\n",
       "      <td>RT @RT_com: LATEST: 108 killed in 7.9-magnitud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sympathy_and_emotional_support</td>\n",
       "      <td>'591902696375877632'</td>\n",
       "      <td>RT @Edourdoo: Shocking picture of the earthqua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>sympathy_and_emotional_support</td>\n",
       "      <td>'591902696895950848'</td>\n",
       "      <td>Indian Air Force is ready to help the people o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                label              tweet_id  \\\n",
       "0          1  infrastructure_and_utilities_damage  '591902695822331904'   \n",
       "1          2               injured_or_dead_people  '591902695943843840'   \n",
       "2          3      missing_trapped_or_found_people  '591902696371724288'   \n",
       "3          4       sympathy_and_emotional_support  '591902696375877632'   \n",
       "4          5       sympathy_and_emotional_support  '591902696895950848'   \n",
       "\n",
       "                                         tweet_texts  \n",
       "0  RT @DailySabah: #LATEST #Nepal's Kantipur TV s...  \n",
       "1  RT @iamsrk: May Allah look after all. Here r t...  \n",
       "2  RT @RT_com: LATEST: 108 killed in 7.9-magnitud...  \n",
       "3  RT @Edourdoo: Shocking picture of the earthqua...  \n",
       "4  Indian Air Force is ready to help the people o...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For my tweets to be informative, there are a few terms I can immediately remove. For instance, any urls won't be useful to the rescue teams. Equally, any '@...' are just calling another twitter handle, and are equally not useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing URLS\n",
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'http\\S+', u'', x))   \n",
    "\n",
    "# removing @... \n",
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'(\\s)@\\w+', u'', x))\n",
    "\n",
    "# removing hashtags\n",
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: re.sub(u'#', u'', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT: LATEST Nepal's Kantipur TV shows at least ...\n",
       "1    RT: May Allah look after all. Here r the emerg...\n",
       "2    RT: LATEST: 108 killed in 7.9-magnitude Nepal ...\n",
       "3    RT: Shocking picture of the earthquake in Nepa...\n",
       "4    Indian Air Force is ready to help the people o...\n",
       "Name: tweet_texts, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.tweet_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alot of `u'RT'` terms in the tweet texts. Since these add nothing to the content of a tweet, I'm just going to get rid of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.tweet_texts = tweets.tweet_texts.apply(lambda x: x.replace(u'RT', u''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting information from tokens using NLTK is a little trickier than with SpaCy (but not much, and NLTK has the advantage of having a twitter specific tokenizer). \n",
    "\n",
    "To extract the pos tags, I use the `pos_tag` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk_tweets = []\n",
    "\n",
    "for tweet in tweets.tweet_texts:\n",
    "    nltk_tweets.append(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u':',\n",
       " u'Over',\n",
       " u'110',\n",
       " u'killed',\n",
       " u'in',\n",
       " u'earthquake',\n",
       " u':',\n",
       " u'Nepal',\n",
       " u'Home',\n",
       " u'Ministry',\n",
       " u'(',\n",
       " u'PTI',\n",
       " u')']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tweets[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, to get the part of speech tags for all of these tokenized tweets, I'll use `nltk.pos_tag`. This method requires a particular file, `'taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle'`, which I can download using `nltk.download()`. \n",
    "\n",
    "Note - this opens a graphic interface, which can then be used to download a host of additional packages which complement NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having downloaded the average perceptron tagger, I can now tag the tweets with each token's part of speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u':', ':'),\n",
       " (u'Over', 'IN'),\n",
       " (u'110', 'CD'),\n",
       " (u'killed', 'VBN'),\n",
       " (u'in', 'IN'),\n",
       " (u'earthquake', 'NN'),\n",
       " (u':', ':'),\n",
       " (u'Nepal', 'NNP'),\n",
       " (u'Home', 'NNP'),\n",
       " (u'Ministry', 'NNP'),\n",
       " (u'(', '('),\n",
       " (u'PTI', 'NNP'),\n",
       " (u')', ')')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(nltk_tweets[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_pos = []\n",
    "\n",
    "for tweet in nltk_tweets:\n",
    "    nltk_pos.append(pos_tag(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, I just need to get the entities from each token, using `ne_chunk`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract entities, I am going to use Stanford's Named Entity Recognizer (NER). \n",
    "\n",
    "I begin by initializing the NER, using files I downloaded from [Stanford's website](https://nlp.stanford.edu/software/CRF-NER.shtml). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('NLTK_resources/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "           'NLTK_resources/stanford-ner-2017-06-09/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since the Stanford NER tagger takes 2 hours to run, I've commented out the code block, and instead saved the content words from each tweet as a `.npy` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_ents = np.load('ner_content.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2337/2337 [1:58:44<00:00,  3.28s/it]  \n"
     ]
    }
   ],
   "source": [
    "# nltk_ents = []\n",
    "\n",
    "# for tweet in tqdm(nltk_tweets): \n",
    "#     entity_tagged_tweet = st.tag(tweet)\n",
    "#     nltk_ents.append([tag for tag in entity_tagged_tweet if (tag[1] != u'O')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all tweets are equally useful. Some just contain prayers, such as\n",
    "\n",
    "`Hope it doesn't rain. #Nepal`\n",
    "\n",
    "whereas others are dense with useful information: \n",
    "\n",
    "`2 Dead, 100 Injured in Bangladesh From Nepal Quake`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I decide which parts of these tweets are most useful? One way to do it is to measure the term frequency-inverse document frequency (tf-idf) of each of the words in the corpus of tweets. This metric measures how important a word is in a corpus of tweets. \n",
    "\n",
    "## Getting the tf-idf values of content words. \n",
    "\n",
    "I can do a preliminary 'cleanup', by keeping only 'content words'. These are defined as : Numerals, Nouns and Verbs. Conveniently, I have just calculated those using NLTK's methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I care most about tokens which are entities, and numbers. The other tokens have too much noise, so let's focus on these two. \n",
    "\n",
    "In addition, entities which are people tend to be personal wishes (non situational), so I remove those from my content tweets as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2337/2337 [00:00<00:00, 96838.49it/s]\n"
     ]
    }
   ],
   "source": [
    "content_tweets = []\n",
    "for pos_tweet, content_words in tqdm(zip(nltk_pos, nltk_ents)):\n",
    "    # we'll start by definitely appending all of the entities\n",
    "    single_tweet_content = [word[0] for word in content_words if word[1] != u'PERSON']\n",
    "    \n",
    "    # next, add the token if it is a number\n",
    "    for token in pos_tweet: \n",
    "        if token[1] == u'CD': # CD = cardinal number\n",
    "            single_tweet_content.append(token[0])\n",
    "    content_tweets.append(single_tweet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_tweet \n",
      "[u'MEA', u'opens', u'24', u'hour', u'Control', u'Room', u'for', u'queries', u'regarding', u'the', u'Nepal', u'Earthquake', u'.', u'\\xe5\\xca', u'Numbers', u':', u'+', u'91', u'11', u'2301', u'2113', u'+', u'91', u'11', u'2301', u'4104', u'+', u'91', u'11', u'2301', u'7905']\n",
      "\n",
      "content_tweet\n",
      "[u'Nepal', u'24', u'91', u'11', u'2301', u'2113', u'91', u'11', u'2301', u'4104', u'91', u'11', u'2301', u'7905']\n"
     ]
    }
   ],
   "source": [
    "tweet_num = 200\n",
    "print (\"original_tweet \\n\" + str(nltk_tweets[tweet_num]) \n",
    "       + \"\\n\\ncontent_tweet\\n\" + str(content_tweets[tweet_num])\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this has already gone some way to (crudely) isolating the interesting parts of a tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, NLTK doesn't calculate tf-idf score automatically. There IS a library which can do this: [textacy](https://textacy.readthedocs.io/en/latest/index.html). Note: textacy is built on SpaCy.\n",
    "\n",
    "I care about the tf-idf scores of the entire tweet, so will find the tf-idf score across the entire corpus of original tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(weighting = 'tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the tf-idf score of all the tokens in the tweets, I can use `fit_transform()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_matrix = vectorizer.fit_transform(nltk_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is a term-document matrix. What this means is that on top of having the tf-idf values, each row is a document (and each column is a word). \n",
    "\n",
    "If the tweet in row `i` contains the column in row `j`, then the element `matrix[i][j]` will contain the tf-idf value. If the tweet *doesn't* contan the word, the matrix value will be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_matrix = term_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2337, 2847)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My ultimate goal is to create a dictionary, which maps from the tokens in the content tweets to some tf-idf score. To do this, I need to find out which tokens are at what columns in the term matrix. \n",
    "\n",
    "The vectorizer object has a dictionary, which maps each token to its column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indo-Nepal 1201\n",
      "Initial 2176\n",
      "Injured 97\n",
      "Injuries 1607\n",
      "Instructed 1942\n",
      "Int'l 1691\n",
      "Intensity 530\n",
      "Interior 2281\n",
      "International 2712\n",
      "Ireland 1922\n",
      "Is 1613\n",
      "Islamabad 1407\n",
      "It 393\n",
      "It's 1147\n",
      "Italian 1972\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(vectorizer.vocabulary)[700:715]:\n",
    "    print key, vectorizer.vocabulary[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And each column (word) has a unique tf-idf value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can therefore map the value of the content tokens to their tf-idf, using the `vectorizer.vocabulary` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Kathmandu', 93, 8.3727132087249654)\n",
      "(u'977 9851', 284, 11.522637736956046)\n",
      "(u'1', 285, 15.873945717696863)\n",
      "(u'07021', 1112, 6.0490009409298038)\n",
      "(u'977 9851', 284, 11.522637736956046)\n",
      "(u'1', 285, 15.873945717696863)\n",
      "(u'35141', 287, 5.761318868478023)\n"
     ]
    }
   ],
   "source": [
    "for token in content_tweets[500]:\n",
    "    print (token, vectorizer.vocabulary[token], np.max(np_matrix[:,vectorizer.vocabulary[token]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_dict = {}\n",
    "content_vocab = []\n",
    "for tweet in content_tweets: \n",
    "    for token in tweet: \n",
    "        if token not in tfidf_dict: \n",
    "            if token in vectorizer.vocabulary:\n",
    "                content_vocab.append(token)\n",
    "                tfidf_dict[token] = np.max(np_matrix[:,vectorizer.vocabulary[token]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD:AssociatedPress -- tf-idf SCORE:8.06390396147\n",
      "WORD:Avalanche -- tf-idf SCORE:5.01938152375\n",
      "WORD:BBC -- tf-idf SCORE:12.3842035691\n",
      "WORD:BIHAR -- tf-idf SCORE:6.9652916728\n",
      "WORD:Bachchan -- tf-idf SCORE:8.06390396147\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(tfidf_dict)[205:210]:\n",
    "    print (\"WORD:\" + str(key) + \" -- tf-idf SCORE:\" +  str(tfidf_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## COntent Word-based Tweet Summarization (COWTS) \n",
    "As per [Rudra et al](http://dl.acm.org/citation.cfm?id=2806485). \n",
    "\n",
    "I'll be using [PyMathProg](http://pymprog.sourceforge.net/index.html) as my Integer Linear Programming Solver. This is a python interface for [GLPK](https://www.gnu.org/software/glpk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymprog import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to maximize \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^n x_{i} + \\sum_{j = 1}^{m} Score(j) \\cdot y_{j}\n",
    "\\end{equation}\n",
    "Where $x_{i}$ is 1 if I include tweet i, or 0 if I don't, and where $y_{j}$ is 1 or 0 if each content word is included (and Score(j) is that word's tf-idf score). \n",
    "\n",
    "I'm going to subject this equation to the following constraints: \n",
    "\n",
    "1. \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}\n",
    "I want the total length of all the selected tweets to be less than some value L, which will be the length of my summary, L. I can vary L depending on how long I want my summary to be. \n",
    "\n",
    "2. \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}\n",
    "If I pick some content word $y_{j}$ (out of my $m$ possible content words) , then I want to have at least one tweet from the set of tweets which contain that content word, $T_{j}$. \n",
    "\n",
    "3. \n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}\n",
    "If I pick some tweet i (out of my $n$ possible tweets) , then all the content words in that tweet $C_{i}$ are also selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWTS') is the default model."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin('COWTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0 <= x[1000] <= 1 binary"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining my first variable, x \n",
    "# This defines whether or not a tweet is selected\n",
    "x = var('x', len(nltk_tweets), bool)\n",
    "\n",
    "# Check this worked\n",
    "x[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Also defining the second variable, which defines\n",
    "# whether or not a content word is chosen\n",
    "y = var('y', len(content_vocab), bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, 0 <= y[0] <= 1 binary)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y), y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have defined my variables, I can define the equation I am maximizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maximize(sum(x) + sum([tfidf_dict[content_vocab[j]]*y[j] for j in range(len(y))]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can define my constraints. First, \n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} x_{i} \\cdot Length(i) \\leq L\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Maximum length of the entire tweet summary\n",
    "\n",
    "# Was 150 for the tweet summary, \n",
    "# But generated a 1000 word summary for CONABS\n",
    "L = 150\n",
    "\n",
    "# hiding the output of this line since its a very long sum \n",
    "sum([x[i]*len(nltk_tweets[i]) for i in range(len(x))]) <= L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next two constraints are slightly more tricky, as I need a way to define which content words are in which tweets. \n",
    "\n",
    "However, the term matrix I defined using the vectorizer has all of this information. \n",
    "\n",
    "I'll begin by defining two helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_words(i):\n",
    "    '''Given a tweet index i (for x[i]), this method will return the indices of the words in the \n",
    "    content_vocab[] array\n",
    "    Note: these indices are the same as for the y variable\n",
    "    '''\n",
    "    tweet = nltk_tweets[i]\n",
    "    content_indices = []\n",
    "    \n",
    "    for token in tweet:\n",
    "        if token in content_vocab:\n",
    "            content_indices.append(content_vocab.index(token))\n",
    "    return content_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_with_content_words(j):\n",
    "    '''Given the index j of some content word (for content_vocab[j] or y[j])\n",
    "    this method will return the indices of all tweets which contain this content word\n",
    "    '''\n",
    "    content_word = content_vocab[j]\n",
    "    \n",
    "    index_in_term_matrix = vectorizer.vocabulary[content_word]\n",
    "    \n",
    "    matrix_column = np_matrix[:, index_in_term_matrix]\n",
    "    \n",
    "    return np.nonzero(matrix_column)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now define the second constraint: \n",
    "\\begin{equation}\n",
    "\\sum_{i \\in T_{j}} x_{i} \\geq y_{j}, j = [1,...,m]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(len(y)):\n",
    "    sum([x[i] for i in tweets_with_content_words(j)])>= y[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the third constraint:\n",
    "\\begin{equation}\n",
    "\\sum_{j \\in C_{i}} y_{j} \\leq |C_{i}| \\times x_{i}, i = [1,...,n]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    sum(y[j] for j in content_words(i)) >= len(content_words(i))*x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.) \\nThe MIP problem instance has been successfully solved. (This code\\ndoes {\\\\it not} necessarily mean that the solver has found optimal\\nsolution. It only means that the solution process was successful.)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_x =  [value.primal for value in x]\n",
    "result_y = [value.primal for value in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model('COWTS') is not the default model."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_tweets = np.nonzero(result_x)\n",
    "chosen_words = np.nonzero(result_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 67)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chosen_tweets[0]), len(chosen_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "MEA opens 24 hour Control Room in Delhi for queries regarding the Nepal Earthquake . 011 2301 2113 011 2301 4104 011 2301 7905\n",
      "--------------\n",
      ": At least 150 people killed in Kathmandu , 1 in China , 10 in Pokhara , 2 at Mount Everest Base Camp and 11 in India frm eart  Û_\n",
      "--------------\n",
      ": USGS reports a M5 earthquake 31km NNW of Nagarkot , Nepal on 4/25 / 15 @ 9:30 : 29 UTC quake\n",
      "--------------\n",
      "Chitttt\n",
      "--------------\n",
      "Nepal's Home Ministry Says at Least 71 People Killed in the Earthquake\n",
      "--------------\n",
      "Raw : Powerful Earthquake Rocks Nepal AssociatedPress Associated Press news\n",
      "--------------\n",
      "Strong Earthquake Strikes Nepal Near Its Capital , Katmandu - New York Times\n",
      "--------------\n",
      ": .. Indian Embassy Helpline in Nepal + 9779851107 021\n",
      "--------------\n",
      ": Patan Durbar Square after earthquake\n",
      "--------------\n",
      ": 6 NDRF teams leave for Nepal and 5 NDRF teams leave for North Bihar . NepalEarthquake\n",
      "--------------\n",
      "Strong earthquake strikes Nepal - BBC News\n"
     ]
    }
   ],
   "source": [
    "for i in chosen_tweets[0]:\n",
    "    print ('--------------')\n",
    "    print \" \".join(nltk_tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the NLTK tokens are harder to sort than the SpaCy ones (for instance, I can't isolate the kind of entities I am interested in), this method is actually less successful than the SpaCy tweets, even though the actual tokenizer is much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_tweets = np.random.choice(nltk_tweets, size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      ": 1934 :: Earthquake in Bihar and Nepal . With 8.0 Magnitude , epicenter was located in Eastern Nepal Near Mount Everest  Û_\n",
      "--------\n",
      ": More than 100 killed in powerful Nepal earthquake , say government officials and police\n",
      "--------\n",
      ": Strong Earthquake Strikes Nepal Near Its Capital , Katmandu\n",
      "--------\n",
      ": prayers go out to all those affected by the earthquake ... _Ù ÷ Ó earthquake Nepal\n",
      "--------\n",
      ":  ÛÏ @googleindia : We  Ûªve just launched a Person Finder instance to help track missing persons for the Nepal earthquake  ÛÓ >\n",
      "--------\n",
      ": BREAKING : More than 100 killed in Nepal earthquake , says interior ministry -\n",
      "--------\n",
      ": Here are the emergency contact numbers for Nepal , share , help . Our prayers are with all in Nepal .\n",
      "--------\n",
      ": We've just launched a Person Finder instance to help track missing persons for the Nepal earthquake -->\n",
      "--------\n",
      ": People are praying to temples in Nepal , even as they rock with aftershocks according to Americans in NepalEarthquake\n",
      "--------\n",
      ": More than 100 killed in powerful Nepal earthquake , say government officials and police\n",
      "--------\n",
      ": Collapsed buildings at Lalitpur , on the outskirts of Kathmandu NepalQuake\n"
     ]
    }
   ],
   "source": [
    "for i in random_tweets:\n",
    "    print ('--------')\n",
    "    print \" \".join(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief comparison does indicate that this method is far better than random choice at providing a situational overview. \n",
    "\n",
    "It's worth noting that even a random distribution will contain a fair amount of information, because of the selective nature in which we isolated tweets; this is already a subsample which contains a higher % of relevant information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is getting long, so I'm going to save these tweets (which I will continue using) and start a fresh notebook for the next steps. \n",
    "\n",
    "## Saving everything for a fresh notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('term_matrix_nltk.npy', np_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('vocab_to_idx_nltk.npy', vectorizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('content_vocab_nltk.npy', content_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('ner_content.npy', nltk_ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
