from torch import nn
from pathlib import Path

from .base import ModelBase


class ConvModel(ModelBase):
    """
    A PyTorch replica of the CNN structured model from the original paper. Note that
    this class assumes feature_engineering was run with channels_first=True

    Parameters
    ----------
    in_channels: int, default=9
        Number of channels in the input data. Default taken from the number of bands in the
        MOD09A1 + the number of bands in the MYD11A2 datasets
    dropout: float, default=0.25
        Default taken from the original repository
    out_channels_list: list or None, default=None
        Out_channels of all the convolutional blocks. If None, default values will be taken
        from the paper. Note the length of the list defines the number of conv blocks used.
    stride_list: list or None, default=None
        Strides of all the convolutional blocks. If None, default values will be taken from the paper
        If not None, must be equal in length to the out_channels_list
    dense_features: list, or None, default=None.
        output feature size of the Linear layers. If None, default values will be taken from the paper.
        The length of the list defines how many linear layers are used.
    savedir: pathlib Path, default=Path('data/models')
        The directory into which the models should be saved.
    """

    def __init__(self, in_channels=9, dropout=0.25, out_channels_list=None, stride_list=None,
                 dense_features=None, savedir=Path('data/models')):
        super().__init__(savedir)
        self.model = ConvNet(in_channels=in_channels, dropout=dropout,
                             out_channels_list=out_channels_list, stride_list=stride_list,
                             dense_features=dense_features)


class ConvNet(nn.Module):
    """
    A crop yield conv net.

    For a description of the parameters, see the ConvModel class.
    """
    def __init__(self, in_channels=9, dropout=0.25, out_channels_list=None, stride_list=None,
                 dense_features=None):
        super().__init__()

        # default values taken from the paper
        if out_channels_list is None:
            out_channels_list = [128, 256, 256, 512, 512, 1024]
        out_channels_list.insert(0, in_channels)

        if stride_list is None:
            stride_list = [1, 2, 1, 2, 1, 2]
        stride_list.insert(0, 0)

        if dense_features is None:
            dense_features = [1024, 1]
        dense_features.insert(0, out_channels_list[-1])

        assert len(stride_list) == len(out_channels_list), \
            "Stride list and out channels list must be the same length!"

        self.convblocks = nn.ModuleList([
            ConvBlock(in_channels=out_channels_list[i-1], out_channels=out_channels_list[i],
                      kernel_size=3, stride=stride_list[i],
                      dropout=dropout) for i in range(1, len(stride_list))
        ])

        self.dense_layers = nn.ModuleList([
            nn.Linear(in_features=dense_features[i-1],
                      out_features=dense_features[i]) for i in range(1, len(dense_features))
        ])
        self.dense = nn.Linear(in_features=1024, out_features=1024)

        self.initialize_weights()

    def initialize_weights(self):
        for convblock in self.convblocks:
            nn.init.kaiming_uniform_(convblock.conv.weight.data)
        for dense_layer in self.dense_layers:
            nn.init.kaiming_uniform_(dense_layer.weight.data)

    def forward(self, x, return_last_dense=False):
        """
        If return_last_dense is true, the feature vector generated by the second to last
        dense layer will also be returned. This is then used to train a Gaussian Process model.
        """
        for block in self.convblocks:
            x = block(x)
        x = x.squeeze(-1).squeeze(-1)

        for layer_number, dense_layer in enumerate(self.dense_layers):
            x = dense_layer(x)
            if return_last_dense and (layer_number == len(self.dense_layers) - 2):
                output = x
        if return_last_dense:
            return x, output
        return x


class ConvBlock(nn.Module):
    """
    A 2D convolution, followed by batchnorm, a ReLU activation, and dropout
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride, dropout):
        super().__init__()
        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                              kernel_size=kernel_size, stride=stride)
        self.batchnorm = nn.BatchNorm2d(num_features=out_channels)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.batchnorm(self.relu(self.conv(x)))
        return self.dropout(x)
